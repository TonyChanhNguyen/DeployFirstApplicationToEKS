[
{
	"uri": "/2-prerequiste/2.1-createcloud9workspace/",
	"title": "Create Cloud9 workspace",
	"tags": [],
	"description": "",
	"content": "Create Cloud9 workspace   Go to Cloud9 at region ap-southeast-1.\n  Click on Create environment.   At Create environment page, input FCJ-Workspace at Name field.\n  Input Workspace for hands on workshop at Description field.\n  At Environment type field, keep default New EC2 instance.\n  At Instance type field, select Additional instance types.\n  At Additional instance types field, select t3.large.   Scroll down to the end of page and click on Create.   The workspace instance is being created.   It will take you about 2 minutes for the instance is created successfully.\n  After the instance is created successfully, click on Open to start your workspace.   "
},
{
	"uri": "/4-interactpcr/4.1-createdockerhubacc/",
	"title": "Create DockerHub account",
	"tags": [],
	"description": "",
	"content": "Docker Hub is a container registry built for developers and open source contributors to find, use, and share their container images. With Hub, developers can host public repos that can be used for free, or private repos for teams and enterprises.\n Access to DockerHub. Click on Sign up to registry account. Then log in to your account.  After log in, you will be requested to fill Username. This is an unique value. Then, click on Sign Up.   At Repository tab, click on Create repository to store your image.   Fill the Repository Name with value mybasicapp.\n  Then click on Create.\n  Next, we will create an access token used to log in DockerHub from workspace instance.\n Click on your avatar (on the page top right side).\n  Click on My Account.\n  Click on Security.\n  Finally, click on New Access Token.   Fill the Access Token Description.\n  Click on Generate.   Save your access token.\n  Back to terminal of Cloud9. Enter this command to login and provide password when be requested.  docker login -u \u0026lt;REPLACE-YOUR-DOCKERHUB-USERNAME\u0026gt; "
},
{
	"uri": "/",
	"title": "Deploy first application on Amazon EKS",
	"tags": [],
	"description": "",
	"content": "Deploy first application on Amazon EKS Overview In this hands-on lab, we will learn fundamental knowledge and how can deploy a basic application on Docker, Kubernetes and Amazon EKS.\nContent  Introduction Prerequisites  2.1 Create Cloud9 workspace 2.2 Modify IAM Role 2.3 Installation 2.4 Create basic application   Deploy application with Docker  3.1 Install Docker 3.2 Create Dockerfile 3.3 Create container image 3.4 Deploy application   Interact with Public Container Registry  4.1 Create DockerHub account 4.2 Push container image to DockerHub 4.3 Pull container image from DockerHub   Deploy application to Kubernetes  5.1 Install Kubernetes 5.2 Deploy application with Kubernetes POD by kubectl 5.3 Deploy application with Kubernetes POD by YAML manifest file   Deploy applicaiton with Amazon EKS  6.1 Install eksclt 6.2 Deploy application by EKS Cluster Managed nodegroup   Cleanup resources  "
},
{
	"uri": "/6-deploytoeks/6.1-installeks/",
	"title": "Install Amazon EKS",
	"tags": [],
	"description": "",
	"content": "eksctl is a simple command line utility for creating and managing Kubernetes clusters on Amazon EKS.\n At Cloud9 terminal, execute those command to install Amazon EKS.   Download and extract the latest release of eksctl with the following command.  curl --silent --location \u0026#34;https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\u0026#34; | tar xz -C /tmp  Move the extracted binary to /usr/local/bin.  sudo mv /tmp/eksctl /usr/local/bin  Test that your installation was successful with the following command  eksctl version "
},
{
	"uri": "/3-deployappwithdocker/3.1-installdocker/",
	"title": "Install Docker",
	"tags": [],
	"description": "",
	"content": "In this step, we will check the version of Docker and install if it is not existing.\nCheck the version of Docker  At the Cloud9 terminal, execute this command to check the version of docker.  docker version At you see, the version of Docker now is 25.0.3.\nRun this command to test with Docker.  sudo docker run hello-world Install Docker if it is not existing.  Update the installed packages and package cache on your instance.  sudo yum update -y Install the most recent Docker Community Edition package.  sudo yum install -y docker 3. Start the Docker service.\nsudo service docker start Add the ec2-user to the docker group so you can execute Docker commands without using sudo.  sudo usermod -a -G docker ec2-user Run this command to test with Docker.  sudo docker run hello-world "
},
{
	"uri": "/5-deploytok8s/5.1-installk8s/",
	"title": "Install Kubernetes tools",
	"tags": [],
	"description": "",
	"content": "First, we need to install Kubernetes tool (Minikube) to your workspace instance. Minikube is a tool that lets you run a single-node Kubernetes cluster on your personal computer or in EC2 instance so that you can try out Kubernetes.\nMinikube is a lightweight Kubernetes (K8) installation, which can create a Virtual Machine (VM) on your local instance or in a cloud instance, which deploys a simple cluster containing only one node.\nBut to install kubectl and minikube on your workspace instance, its storage capacity must be redundant. So first, we need to expand the storage capacity of workspace instance.\nExpand storage capacity to 50GB.   Go to EC2 instance of your workspace instance.   Navigate to Storage tab.\n  Click on Volume ID.   Your storage capacity now is 10GB. We will upgrade it to 50GB. 4. Select your EBS volume. 5. Click on Action. Then select Modify volume. 6. Change from 10 to 50 at Size (GiB) field. 7. Click on Modify.  Then, click on Modify to confirm.   Go back to Cloud9 terminal, execute the below command to reboot your workspace instance.\n  sudo reboot After workspace instance reboot successfully. Check the result.  df -h There is a volume mounted to your workspace instance with capacity is 43GB. Install kubectl  At Cloud9 Terminal, execute those command to install kubectl.   Update the instance packages.  sudo yum update  Install kubectl.  curl -LO https://dl.k8s.io/release/v1.30.0/bin/linux/amd64/kubectl sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl Check version of kubectl.  kubectl version --client Install minikube  At Cloud9 Terminal, execute those command to install minikube.  curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64\rsudo install minikube-linux-amd64 /usr/local/bin/minikube \u0026amp;\u0026amp; rm minikube-linux-amd64 Start Kubernetes Cluster with minikube.  minikube start Check the version.  minikube version "
},
{
	"uri": "/1-introduce/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Docker Overview Docker is an open platform for developing, shipping, and running applications. Docker enables you to separate your applications from your infrastructure so you can deliver software quickly. With Docker, you can manage your infrastructure in the same ways you manage your applications. By taking advantage of Docker\u0026rsquo;s methodologies for shipping, testing, and deploying code, you can significantly reduce the delay between writing code and running it in production.\nDocker architecture The Docker daemon: The Docker daemon listens for Docker API requests and manages Docker objects such as images, containers, networks, and volumes. A daemon can also communicate with other daemons to manage Docker services.\nThe Docker client: The Docker client is the primary way that many Docker users interact with Docker. When you use commands, the client sends these commands to Docker daemon, which carries them out. The docker command uses the Docker API. The Docker client can communicate with more than one daemon.\nDocker Desktop: Docker Desktop is an easy-to-install application for your Mac, Windows or Linux environment that enables you to build and share containerized applications and microservices.\nDocker registries: A Docker registry stores Docker images. Docker Hub is a public registry that anyone can use, and Docker looks for images on Docker Hub by default.\nImages: An image is a read-only template with instructions for creating a Docker container.\nContainers: A container is a runnable instance of an image. You can create, start, stop, move, or delete a container using the Docker API or CLI.\nAccess to Docker docs for more detail.\n\rKubernetes Overview Kubernetes is a portable, extensible, open source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.\nKubernetes cluster architecture Cluster: A Kubernetes cluster consists of a set of worker machines, called nodes, that run containerized applications. Every cluster has at least one worker node.\nControl Plane: The control plane manages the worker nodes and the Pods in the cluster. In production environments, the control plane usually runs across multiple computers and a cluster usually runs multiple nodes, providing fault-tolerance and high availability.\nWorker Nodes: The worker node(s) host the Pods that are the components of the application workload\nkube-api-server: The API server is a component of the Kubernetes control plane that exposes the Kubernetes API. The API server is the front end for the Kubernetes control plane.\netcd: Consistent and highly-available key value store used as Kubernetes backing store for all cluster data.\nScheduler: Control plane component that watches for newly created Pods with no assigned node, and selects a node for them to run on.\nController Manager: Control plane component that runs controller processes. Logically, each controller is a separate process, but to reduce complexity, they are all compiled into a single binary and run in a single process. There are many different types of controllers. Some examples of them are:\n Node controller: Responsible for noticing and responding when nodes go down. Job controller: Watches for Job objects that represent one-off tasks, then creates Pods to run those tasks to completion. EndpointSlice controller: Populates EndpointSlice objects (to provide a link between Services and Pods). ServiceAccount controller: Create default ServiceAccounts for new namespaces.  The above is not an exhaustive list.\nkubelet: An agent that runs on each node in the cluster. It makes sure that containers are running in a Pod.\nkube-proxy: kube-proxy is a network proxy that runs on each node in your cluster, implementing part of the Kubernetes Service concept.\npod: is a single instance of an application and the smallest object, that you can create in Kubernetes.\nContainer Runtime Interface (CRI): A fundamental component that empowers Kubernetes to run containers effectively. It is responsible for managing the execution and lifecycle of containers within the Kubernetes environment.\ncloud-control-manager: A Kubernetes control plane component that embeds cloud-specific control logic. The cloud controller manager lets you link your cluster into your cloud provider\u0026rsquo;s API, and separates out the components that interact with that cloud platform from components that only interact with your cluster.\nAccess to Kubernetes docs for more detail.\n\rAmazon EKS Overview Amazon Elastic Kubernetes Service (Amazon EKS) is a managed service that eliminates the need to install, operate, and maintain your own Kubernetes control plane on Amazon Web Services (AWS)\nAmazon EKS architecture Amazon EKS ensures every cluster has its own unique Kubernetes control plane. This design keeps each cluster\u0026rsquo;s infrastructure separate, with no overlaps between clusters or AWS accounts. The setup includes:\n Distributed components: The control plane positions at least two API server instances and three etcd instances across three AWS Availability Zones within an AWS Region. Optimal performance: Amazon EKS actively monitors and adjusts control plane instances to maintain peak performance. Resilience: If a control plane instance falters, Amazon EKS quickly replaces it, using different Availability Zone if needed. Consistent uptime: By running clusters across multiple Availability Zones, a reliable API server endpoint availability Service Level Agreement (SLA) is achieved.  Amazon EKS uses Amazon Virtual Private Cloud (Amazon VPC) to limit traffic between control plane components within a single cluster. Cluster components can\u0026rsquo;t view or receive communication from other clusters or AWS accounts, except when authorized by Kubernetes role-based access control (RBAC) policies.\nIn addition to the control plane, an Amazon EKS cluster has a set of worker machines called nodes. Selecting the appropriate Amazon EKS cluster node type is crucial for meeting your specific requirements and optimizing resource utilization. Amazon EKS offers the following primary node types:\n AWS Fargate: Fargate is a serverless compute engine for containers that eliminates the need to manage the underlying instances. With Fargate, you specify your application\u0026rsquo;s resource needs, and AWS automatically provisions, scales, and maintains the infrastructure. This option is ideal for users who prioritize ease-of-use and want to concentrate on application development and deployment rather than managing infrastructure. Karpenter: Karpenter is a flexible, high-performance Kubernetes cluster autoscaler that helps improve application availability and cluster efficiency. Karpenter launches right-sized compute resources in response to changing application load. This option can provision just-in-time compute resources that meet the requirements of your workload. Managed node groups: Managed node groups are a blend of automation and customization for managing a collection of Amazon EC2 instances within an Amazon EKS cluster. AWS takes care of tasks like patching, updating, and scaling nodes, easing operational aspects. In parallel, custom kubelet arguments are supported, opening up possibilities for advanced CPU and memory management policies. Moreover, they enhance security via AWS Identity and Access Management (IAM) roles for service accounts, while curbing the need for separate permissions per cluster. Self-managed nodes: Self-managed nodes offer full control over your Amazon EC2 instances within an Amazon EKS cluster. You are in charge of managing, scaling, and maintaining the nodes, giving you total control over the underlying infrastructure. This option is suitable for users who need granular control and customization of their nodes and are ready to invest time in managing and maintaining their infrastructure.   Access to Amazon EKS docs for more detail.\n\r"
},
{
	"uri": "/3-deployappwithdocker/3.2-createdockerfile/",
	"title": "Create Dockerfile",
	"tags": [],
	"description": "",
	"content": "In this step, we will create a Dockerfile to guide Docker how to build container image of your application.\n Create a file named Dockerfile.  touch Dockerfile  Open the Dockerfile.   Enter the code below.\n  FROMnode:13-alpine#configure working directoryWORKDIR/appCOPY package.json ./RUN npm install#bundle the source codeCOPY . ./EXPOSE8080CMD [\u0026#34;node\u0026#34;,\u0026#34;index.js\u0026#34;]Note:\n  At step COPY package.json ./, Docker will copy package.json file to working directory /app, then process step RUN npm install to install all dependencies were defined in package.json file and save them to node_modules folder.\n  At step COPY . ./, Docker will copy all resource to working directory /app - include node_modules folder and Dockerfile file. Those folder and file are not necessary to copy again to working directory. So you can create .dockerignore to list which file or folder do not need to copy to working directory.\n  Create file named .dockerignore by command.  touch .dockerignore You will not see where .dockerignore is, since Cloud9 recognize .dockerignore is hidden file.\nClick on Setting. Select Show Hidden Files.  Open .dockerignore and enter those values.  node_modules\rDockerfile "
},
{
	"uri": "/6-deploytoeks/6.2-eksmanagednodegroup/",
	"title": "Deploy application to EKS Cluster Managed Nodegroup",
	"tags": [],
	"description": "",
	"content": "A Kubernetes node is a machine that runs containerized applications. Each node has the following components:\n Container runtime – Software that\u0026rsquo;s responsible for running the containers. kubelet – Makes sure that containers are healthy and running within their associated Pod. kube-proxy – Maintains network rules that allow communication to your Pods.  Your Amazon EKS cluster can schedule Pods on any combination of:\n AWS Fargate: Fargate is a serverless compute engine for containers that eliminates the need to manage the underlying instances. With Fargate, you specify your application\u0026rsquo;s resource needs, and AWS automatically provisions, scales, and maintains the infrastructure. This option is ideal for users who prioritize ease-of-use and want to concentrate on application development and deployment rather than managing infrastructure. Karpenter: Karpenter is a flexible, high-performance Kubernetes cluster autoscaler that helps improve application availability and cluster efficiency. Karpenter launches right-sized compute resources in response to changing application load. This option can provision just-in-time compute resources that meet the requirements of your workload. Managed node groups: Managed node groups are a blend of automation and customization for managing a collection of Amazon EC2 instances within an Amazon EKS cluster. AWS takes care of tasks like patching, updating, and scaling nodes, easing operational aspects. In parallel, custom kubelet arguments are supported, opening up possibilities for advanced CPU and memory management policies. Moreover, they enhance security via AWS Identity and Access Management (IAM) roles for service accounts, while curbing the need for separate permissions per cluster. Self-managed nodes: Self-managed nodes offer full control over your Amazon EC2 instances within an Amazon EKS cluster. You are in charge of managing, scaling, and maintaining the nodes, giving you total control over the underlying infrastructure. This option is suitable for users who need granular control and customization of their nodes and are ready to invest time in managing and maintaining their infrastructure.cheduled on. Amazon EKS nodes run in your AWS account and connect to the control plane of your cluster through the cluster API server endpoint.  But in this workshop, we will only focus on deploying application to EKS Cluster Managed Nodegroup.\nCreate EKS Cluster  At Cloud9 terminal, execute command below to create EKS Cluster.  eksctl create cluster --name=my-fcj-cluster --region=ap-southeast-1 --zones=ap-southeast-1a,ap-southeast-1b --managed --nodegroup-name=my-fcj-node --nodes=2 --node-type=t3.micro It will take you about 20 minutes to finish.\n\r2. After Cluster creation process finish, list your cluster on workspace machine.\neksctl get cluster List Nodes in Cluster.  kubectl get node There are 2 nodes listed in EKS Cluster.\nLet go to EC2 Instance to see the instance associated to Node Group.   There are 2 instances created (match with \u0026ndash;nodes=2 on eksctl command) with name is my-fcj-cluster-my-fcj-node-Node (format --Node), type is t3.micro (match with \u0026ndash;node-type=t3.micro on eksctl command) and Availability Zone is ap-southeast-1a and ap-southeast-1b (match with \u0026ndash;zones=ap-southeast-1a,ap-southeast-1b on eksctl command).\nExplore EKS Cluster   Go to CloudFormation, we will see there are 2 stacks created - 1 for creating EKS Cluster and 1 for EKS NodeGroup.   Go to Amazon EKS, we will see there is a EKS Cluster named my-fcj-cluster. Click on it.   Navigate to Compute tab.\n  We will see the node group named my-fcj-node with Desired size is 2 associated to EKS Cluster at Node groups field.   Deploy application to EKS Cluster  At kube-manifest directory, execute those command to deploy application to EKS Cluster.  kubectl apply -f . List the service created.  kubectl get svc or\nkubectl get service Access to LoadBalancer HostName to see the result. Replace your service\u0026rsquo;s EXTERNAL-IP on http://\u0026lt;REPLACE-YOUR-EXTERNAL-IP\u0026gt;:8080.  Example: In my case the host name will be http://a6d733c6da3c1499f9ac945a665aa815-1253515241.ap-southeast-1.elb.amazonaws.com:8080 Congratulations, you had deployed your application to EKS CLuster Managed Node Group successfully. Clean up.  At Cloud9 terminal, to delete all resources created. Execute command below.  kubectl delete -f . List all resources to make sure that they are deleted totally.  kubectl get all "
},
{
	"uri": "/5-deploytok8s/5.2-deployk8simperative/",
	"title": "Deploy application with Kubernetes POD by kubectl",
	"tags": [],
	"description": "",
	"content": "In previous step, we had installed kubectl and minikube successfully. In this step, we will deploy the application on Kubernetes Deployment by imperative way.\n List all images in your workspace machine.  docker images Execute those command to create a Kubernetes Deployment in Minikube Cluster with your application\u0026rsquo;s container image.  kubectl create deployment mybasicapp --image=firstcloudjourneypcr/mybasicapp:v1 List and check the Deployment named mybasicapp.  kubectl get deployment List POD created by Deployment mybasicapp.  kubectl get pod Now, the POD of application was created but can not be accessed. To access to POD, we need to create the Service.\nExecute the below command to create the Service Load Balancer.  kubectl expose deployment mybasicapp --type=LoadBalancer --port=8080 List and check the Service named mybasicapp.  kubectl get service Create the tunnel to access to your Load Balancer.  minikube tunnel  Click on Window. Select New Terminal to create the separate terminal.   List and check the Service named mybasicapp again.\n  kubectl get service The EXTERNAL-IP appeared.\nNow we will test the application by command.  curl http://\u0026lt;REPLACE-WITH-EXTERNAL-IP\u0026gt;:8080 Congratulations, your application was deployed to Minikube using kubectl successfully. Delete Deployment and Service of your application.  kubectl delete deployment mybasicapp\rkubectl delete service mybasicapp Check Deployment and Service again to make sure they were deleted.  kubectl get all "
},
{
	"uri": "/5-deploytok8s/5.3-deployk8sdeclarative/",
	"title": "Deploy application with Kubernetes POD by YAML manifest file",
	"tags": [],
	"description": "",
	"content": "In previous step, we will deploy the application on Kubernetes Deployment by imperative way. In this step, we will deploy the application on Kubernetes Deployment by declarative way.\n Create a directory named kube-manifest.  mkdir kube-manifest\rcd kube-manifest Create a file named 01.mybasicapp-deployment.yaml.  touch 01.mybasicapp-deployment.yaml Open file 01.mybasicapp-deployment.yaml. Paste the code below and save.  apiVersion: apps/v1\rkind: Deployment\rmetadata:\rname: mybasicapp-deployment\rlabels:\rapp: mybasicapp\rspec:\rreplicas: 1\rselector:\rmatchLabels:\rapp: mybasicapp\rtemplate:\rmetadata:\rlabels:\rapp: mybasicapp\rspec:\rcontainers:\r- name: mybasicapp\rimage: firstcloudjourneypcr/mybasicapp:v1\rports: - containerPort: 8080 Create the file named 02.mybasicapp-service.yaml and save those code below.  apiVersion: v1\rkind: Service\rmetadata:\rname: mybasicapp-service\rlabels:\rapp: mybasicapp\rspec:\rselector:\rapp: mybasicapp\rtype: LoadBalancer\rports:\r- port: 8080\rtargetPort: 8080 Apply those kubernetes file by command.  kubectl apply -f . Then, check Deployment, Service and POD had been created by command.  kubectl get all Test your application.  curl http://\u0026lt;REPLACE-WITH-EXTERNAL-IP\u0026gt;:8080 Congratulations, your application was deployed to Minikube using manifest file successfully. To delete Deployment and Service created by declarative way, using below command.  kubectl delete -f . Check Deployment and Service again to make sure that they were deleted.  kubectl get all Stop your Minikube Cluster to save the resource.  minikube stop "
},
{
	"uri": "/2-prerequiste/2.2-modifyiamrole/",
	"title": "Modify IAM role",
	"tags": [],
	"description": "",
	"content": "In this step, we will create a IAM Role and assign it to workspace instance.\nCreate IAM role   Click IAM to navigate to IAM service.\n  Click on Role.\n  Click on Create role.   At Trusted entity type field, select AWS service.\n  At Service or use case field, select EC2.   Then, click on Next.   At Permissions policies field, select policy name AdministratorAccess.   Then, click on Next.   At Name, review, and create page, input eksworkspace-administrator at Role name field.   Then, scroll down to the end of page and click on Create role.   Assign role to workspace instance   At AWS Cloud9 interface, click on Manage EC2 instance.   You will see the created workspace instance. Then, click to select it.\n  Click on Action.\n  Click on Security.\n  Click on Modify IAM role.   Select the role name eksworkspace-administrator which was created at above steps.\n  Then, click on Update IAM role.   New IAM role was updated successfully.   Update Cloud9 configuration Cloud9 will manage IAM credentials automatically. This default configuration is currently not compatible with EKS authentication via IAM, we will need to disable this feature and use the IAM Role.\n\r  At AWS Cloud9 interface, click on AWS Cloud9.\n  Select Preferences.   At AWS Settings, disable AWS managed temporary credentials.   To ensure that temporary credentials are not saved in Cloud9, we will delete all existing credentials with the command below.\n  rm -vf ${HOME}/.aws/credentials "
},
{
	"uri": "/2-prerequiste/",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": "Overview To conduct the lab, we have to prepare the Cloud9 workspace instance and create the IAM role for the Cloud9 instance.\nContent  2.1 Create Cloud9 workspace 2.2 Modify IAM Role 2.3 Installation 2.4 Create basic application  "
},
{
	"uri": "/4-interactpcr/4.2-pushimagetodockerhub/",
	"title": "Push image to DockerHub",
	"tags": [],
	"description": "",
	"content": "In previous step, we created DockerHub account, public repository and login successfully. Now, we will perform to push container image to created repository.\n Let list the container images in workspace instance again.  docker images At you can see, we had only 1 image with Repository is fcj-application. To push container image to repository, Repository of your container image must match with repository format of DockerHub \u0026lt;YOUR_USER_NAME\u0026gt;/\u0026lt;YOUR_REPOSITORY_NAME\u0026gt;. So now we need to use docker image tag command to copy and correct repository format of your container image in workspace instance.\nExecute this command to tag your container image.  docker image tag fcj-application:v1 firstcloudjourneypcr/mybasicapp:v1 Then, let list container images in workspace instance again to see the result.  docker images Now, there are 2 container images. The new one has repository named firstcloudjourneypcr/mybasicapp with tag is v1. Next, let push this container image to DockerHub.\nExecute this command to push your container image to DockerHub.  docker push firstcloudjourneypcr/mybasicapp:v1 Back to your Docker Hub repository to check the result. There is a image container had been pushed to.  "
},
{
	"uri": "/3-deployappwithdocker/3.3-createdockerimage/",
	"title": "Create container image",
	"tags": [],
	"description": "",
	"content": "You had created Dockerfile - to guide Docker the way to build container image for your application and .dockerignore - to show to Docker know which resources do not need to copy to working directory. Now, we will perform to build container image from those Dockerfile and .dockerignore file.\nBefore build container image. Let check there is any image in your machine, but let check which process is running in your machine first.\nCheck process  Enter this command to list running process in your machine.  docker ps There is no running process in your machine. Now we will check which process is existing in your machine. 2. Enter this command to list existing process in your machine.\ndocker ps -a There are 2 processes are existing in machine but status is Exited. It means those processes had been stopped. Let remove them into your machine. 3. Enter this command to remove all existing process.\ndocker rm $(docker ps -aq) Let check existing process again.  docker ps -a There are no existing process in machine.\nCheck container image  Enter this command to list all container images in your machine.  docker images There is a container image named hello-world. This image was pulled to your machine when you test Docker Engine at step 3.1 Install Docker by command sudo docker run hello-world.\nNow we will delete this image by command.  docker delete image hello-word Let list images again.  docker images Now, there are no images in your machine. Let build container image for your application.\nBuild container image  Execute this command to build container image for your application.  docker build -t fcj-application:v1 . Now, let list all images in your machine.  docker images The container image of your application had been built successfully.\n"
},
{
	"uri": "/3-deployappwithdocker/",
	"title": "Deploy application with Docker",
	"tags": [],
	"description": "",
	"content": "In this section, we will deploy the application using Docker to understand the way to containerize and run application.\nOverview Docker is an open platform for developing, shipping, and running applications. Docker enables you to separate your applications from your infrastructure so you can deliver software quickly. With Docker, you can manage your infrastructure in the same ways you manage your applications. By taking advantage of Docker\u0026rsquo;s methodologies for shipping, testing, and deploying code, you can significantly reduce the delay between writing code and running it in production.\nDocker architecture The Docker daemon: The Docker daemon listens for Docker API requests and manages Docker objects such as images, containers, networks, and volumes. A daemon can also communicate with other daemons to manage Docker services.\nThe Docker client: The Docker client is the primary way that many Docker users interact with Docker. When you use commands, the client sends these commands to Docker daemon, which carries them out. The docker command uses the Docker API. The Docker client can communicate with more than one daemon.\nDocker Desktop: Docker Desktop is an easy-to-install application for your Mac, Windows or Linux environment that enables you to build and share containerized applications and microservices.\nDocker registries: A Docker registry stores Docker images. Docker Hub is a public registry that anyone can use, and Docker looks for images on Docker Hub by default.\nImages: An image is a read-only template with instructions for creating a Docker container.\nContainers: A container is a runnable instance of an image. You can create, start, stop, move, or delete a container using the Docker API or CLI.\nAccess to Docker docs for more detail.\n\rContent  3.1 Install Docker 3.2 Create Dockerfile 3.3 Create container image 3.4 Deploy application  "
},
{
	"uri": "/2-prerequiste/2.3-installation/",
	"title": "Installation",
	"tags": [],
	"description": "",
	"content": "In this workshop, we will create a basic application with NodeJS and Express framework. So we need to check the version of them and install if they are not existing.\nUpgrade awscli  Copy and paste the command below into Terminal of Cloud9 Workspace to upgrade awscli.  sudo pip install --upgrade awscli \u0026amp;\u0026amp; hash -r Check version of npm and node  Copy and Paste the command below into Terminal of Cloud9 Workspace to check version of npm and node.  npm version At you see, npm and node had been installed. Now we will install Express framework by using npm.\nInstall Express framework  Copy and Paste the command below into Terminal of Cloud9 Workspace to install Express framework.  npm install express --save "
},
{
	"uri": "/4-interactpcr/4.3-pullimagefromdockerhub/",
	"title": "Pull image from DockerHub",
	"tags": [],
	"description": "",
	"content": "In previous step, we had pushed container image to DockerHub repository. In this step, we will pull container image to Workspace instance from DockerHub repository. To do that, we wil remove all images in workspace instance first. Then, we will perform pulling and running Docker container application from that image.\n Let list all images in workspace instance.  docker images There are 2 container images in workspace instance. 2. Next, execute this command to delete them. Confirm Y when be asked.\ndocker image prune -a List all images again to make sure they were deleted.  docker images Now we will pull the container image from DockerHub repository by command.  docker pull firstcloudjourneypcr/mybasicapp:v1 Then, list pulled container image  docker images We can see there is a container image hab been pulled successfully.\nNow, we will deploy this image to Docker to run application for testing.  docker run --name demoapplication -d -p 8080:8080 firstcloudjourneypcr/mybasicapp:v1 Check running process in workspace instance.  docker ps The application had been deployed successfully.\nAccess to your application to see the result.   Congratulations, you had pulled container image from Public Container Registry - DockerHub and deploy it to Docker successfully! "
},
{
	"uri": "/2-prerequiste/2.4-createbasicapp/",
	"title": "Create basic application",
	"tags": [],
	"description": "",
	"content": "In this step, we will create a basic application using NodeJS and Express framework.\n At the Cloud9 terminal, enter the command below to create a new directory for application.  mkdir app\rcd app Initialize application.  npm init Press Enter to skip those step and confirm Yes to finish.  Create a file name index.js.  touch index.js Open index.js file and perform code.  import express from \u0026#39;express\u0026#39;  const app = express()  app.get(\u0026#39;/\u0026#39;,(req, res) =\u0026gt; {  res.json(\u0026#34;Hello world from FCJ Workshop V1!\u0026#34;) })  app.listen(8080, ()=\u0026gt; {  console.log(\u0026#34;application running on 8080\u0026#34;) }) Save the file and enter this command in terminal to run the application.  node index.js  But you will get the error below.   To fix this error, open package.json file and add the definition below. Then save it.\n  \u0026#34;type\u0026#34;:\u0026#34;module\u0026#34;, Now, let enter the command to run your application again.  node index.js See, your application ran on port 8080.   Now, we need to access to application to see the result. 12. Click on Share. 13. Copy the IP Address at Application field. Access to the application with URL http://\u0026lt;REPLACE_YOUR_IP\u0026gt;:8080. Opps, the application can not be accessed.   The reason is the security group of workspace instance is still not opened for port 8080.\n Click to R symbol and choose Manage EC2 Instance to go back to workspace instance.   At Instances page, select your workspace instance.\n  Navigate to Security tab.\n  Click to Security Group.   At Inbound rules interface, you can see there are no rules were defined. 20. Click on Edit inbound rules. 21. At Edit inbound rules interface, click on Add rule. 22. Define the rule with parameter:\n Type is Custom TCP. Port range is 8080. Source is Anywhere-IPv4.   Then, click on Save rules.   Now, let access to your application again and see the result.   Congratulation, it works. "
},
{
	"uri": "/3-deployappwithdocker/3.4-deployapp/",
	"title": "Deploy application",
	"tags": [],
	"description": "",
	"content": "In previous step, you had built container image for your application. Now, we will deploy it to Docker.\nDeploy application to Docker using container image  Execute this command to run your application with Docker.  docker run -d --name my-docker-deployment -p 8080:8080 fcj-application:v1 Check which process is running in your machine.  docker ps -a Access to URL http://\u0026lt;YOUR_IP_ADDRESS\u0026gt;:8080 to see the result.   Congratulations, you had deployed your application to Docker successfully. Delete process.  First, you need to stop your process by command.  docker stop my-docker-deployment Then, use this command to remove your process.  docker rm my-docker-deployment Let check existing process in your machine again.  docker ps -a "
},
{
	"uri": "/4-interactpcr/",
	"title": "Interact with Public Container Registry",
	"tags": [],
	"description": "",
	"content": "Overview A Public Container Registry (PCR) is a tool for hosting, versioning, and distributing container image publicly within repositories. DockerHub is one kind of them and free to use. In this lab, we will use DockerHub for Public Container Registry.\nContent  4.1 Create DockerHub account 4.2 Push container image to DockerHub 4.3 Pull container image from DockerHub  "
},
{
	"uri": "/5-deploytok8s/",
	"title": "Deploy application with K8S",
	"tags": [],
	"description": "",
	"content": "Overview Kubernetes is a portable, extensible, open source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.\nKubernetes cluster architecture Cluster: A Kubernetes cluster consists of a set of worker machines, called nodes, that run containerized applications. Every cluster has at least one worker node.\nControl Plane: The control plane manages the worker nodes and the Pods in the cluster. In production environments, the control plane usually runs across multiple computers and a cluster usually runs multiple nodes, providing fault-tolerance and high availability.\nWorker Nodes: The worker node(s) host the Pods that are the components of the application workload\nkube-api-server: The API server is a component of the Kubernetes control plane that exposes the Kubernetes API. The API server is the front end for the Kubernetes control plane.\netcd: Consistent and highly-available key value store used as Kubernetes\u0026rsquo; backing store for all cluster data.\nScheduler: Control plane component that watches for newly created Pods with no assigned node, and selects a node for them to run on.\nController Manager: Control plane component that runs controller processes. Logically, each controller is a separate process, but to reduce complexity, they are all compiled into a single binary and run in a single process. There are many different types of controllers. Some examples of them are:\n Node controller: Responsible for noticing and responding when nodes go down. Job controller: Watches for Job objects that represent one-off tasks, then creates Pods to run those tasks to completion. EndpointSlice controller: Populates EndpointSlice objects (to provide a link between Services and Pods). ServiceAccount controller: Create default ServiceAccounts for new namespaces.  The above is not an exhaustive list.\nkubelet: An agent that runs on each node in the cluster. It makes sure that containers are running in a Pod.\nkube-proxy: kube-proxy is a network proxy that runs on each node in your cluster, implementing part of the Kubernetes Service concept.\npod: is a single instance of an application and the smallest object, that you can create in Kubernetes.\nContainer Runtime Interface (CRI): A fundamental component that empowers Kubernetes to run containers effectively. It is responsible for managing the execution and lifecycle of containers within the Kubernetes environment.\ncloud-control-manager: A Kubernetes control plane component that embeds cloud-specific control logic. The cloud controller manager lets you link your cluster into your cloud provider\u0026rsquo;s API, and separates out the components that interact with that cloud platform from components that only interact with your cluster.\nAccess to Kubernetes docs for more detail.\n\rContent  5.1 Install Kubernetes 5.2 Deploy application with Kubernetes POD by kubectl 5.3 Deploy application with Kubernetes POD by YAML manifest file  "
},
{
	"uri": "/6-deploytoeks/",
	"title": "Deploy application to EKS Cluster",
	"tags": [],
	"description": "",
	"content": "Overview Amazon Elastic Kubernetes Service (Amazon EKS) is a managed service that eliminates the need to install, operate, and maintain your own Kubernetes control plane on Amazon Web Services (AWS)\nAmazon EKS architecture Amazon EKS ensures every cluster has its own unique Kubernetes control plane. This design keeps each cluster\u0026rsquo;s infrastructure separate, with no overlaps between clusters or AWS accounts. The setup includes:\n Distributed components: The control plane positions at least two API server instances and three etcd instances across three AWS Availability Zones within an AWS Region. Optimal performance: Amazon EKS actively monitors and adjusts control plane instances to maintain peak performance. Resilience: If a control plane instance falters, Amazon EKS quickly replaces it, using different Availability Zone if needed. Consistent uptime: By running clusters across multiple Availability Zones, a reliable API server endpoint availability Service Level Agreement (SLA) is achieved.  Amazon EKS uses Amazon Virtual Private Cloud (Amazon VPC) to limit traffic between control plane components within a single cluster. Cluster components can\u0026rsquo;t view or receive communication from other clusters or AWS accounts, except when authorized by Kubernetes role-based access control (RBAC) policies.\nIn addition to the control plane, an Amazon EKS cluster has a set of worker machines called nodes. Selecting the appropriate Amazon EKS cluster node type is crucial for meeting your specific requirements and optimizing resource utilization. Amazon EKS offers the following primary node types:\n AWS Fargate: Fargate is a serverless compute engine for containers that eliminates the need to manage the underlying instances. With Fargate, you specify your application\u0026rsquo;s resource needs, and AWS automatically provisions, scales, and maintains the infrastructure. This option is ideal for users who prioritize ease-of-use and want to concentrate on application development and deployment rather than managing infrastructure. Karpenter: Karpenter is a flexible, high-performance Kubernetes cluster autoscaler that helps improve application availability and cluster efficiency. Karpenter launches right-sized compute resources in response to changing application load. This option can provision just-in-time compute resources that meet the requirements of your workload. Managed node groups: Managed node groups are a blend of automation and customization for managing a collection of Amazon EC2 instances within an Amazon EKS cluster. AWS takes care of tasks like patching, updating, and scaling nodes, easing operational aspects. In parallel, custom kubelet arguments are supported, opening up possibilities for advanced CPU and memory management policies. Moreover, they enhance security via AWS Identity and Access Management (IAM) roles for service accounts, while curbing the need for separate permissions per cluster. Self-managed nodes: Self-managed nodes offer full control over your Amazon EC2 instances within an Amazon EKS cluster. You are in charge of managing, scaling, and maintaining the nodes, giving you total control over the underlying infrastructure. This option is suitable for users who need granular control and customization of their nodes and are ready to invest time in managing and maintaining their infrastructure.   Access to Amazon EKS docs for more detail.\n\rContent  6.1 Install eksclt 6.2 Deploy application by EKS Cluster Managed nodegroup  "
},
{
	"uri": "/7-cleanup/",
	"title": "Cleanup resources",
	"tags": [],
	"description": "",
	"content": "Clean up EKS Cluster  Execute the command below to delete EKS Cluster  eksctl delete cluster --name my-fcj-cluster --region ap-southeast-1 It will take you about 20 minutes to delete.\n\rDelete Cloud9 Workspace  Go to Cloud9. Select FCJ-Workspace. Click on Delete.  Input Delete to confirm. Click on Delete.   Delete IAM Role.   Go to IAM Role.\n  Search IAM Role named eksworkspace-administrator.\n  Select IAM Role.\n  Click on Delete.   Input the IAM Role\u0026rsquo;s name eksworkspace-administrator to confirm.\n  Click on Delete.   "
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]